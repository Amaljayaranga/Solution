{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution_1.ipynb",
      "provenance": [],
      "mount_file_id": "1BsG2pCZvuiLivtLWK87xhvAfY77_g-zt",
      "authorship_tag": "ABX9TyOVnhTf70qK8voKMDxgQFXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amaljayaranga/Solution/blob/master/Solution_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys7Z7zExKkAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "    img = Image.open(img_path)\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = transform(img)\n",
        "    pred = model([img])\n",
        "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
        "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
        "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
        "    pred_boxes = pred_boxes[:pred_t + 1]\n",
        "    pred_class = pred_class[:pred_t + 1]\n",
        "    return pred_boxes, pred_class\n",
        "\n",
        "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(boxes)):\n",
        "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
        "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "  plt.figure(figsize=(20,30))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "object_detection_api('00000.jpg', threshold=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXEJgSFyP8Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip 'People.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwcWxauzRYUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6847a2e1-9056-4f1d-c8b1-6435dc55588b"
      },
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import torch.nn as nn\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "    img = Image.open(img_path)\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = transform(img)\n",
        "    pred = model([img])\n",
        "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
        "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
        "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
        "    pred_boxes = pred_boxes[:pred_t + 1]\n",
        "    pred_class = pred_class[:pred_t + 1]\n",
        "    return pred_boxes, pred_class\n",
        "\n",
        "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(boxes)):\n",
        "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
        "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "  plt.figure(figsize=(20,30))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "#object_detection_api('./bird.jpg', threshold=0.5)\n",
        "#boxes, pred_cls = get_prediction('./2.jpg', threshold=0.6)\n",
        "#print(pred_cls)\n",
        "#print(boxes)\n",
        "\n",
        "img_data = {}\n",
        "img_data['imgs'] = []\n",
        "path = './People/'\n",
        "\n",
        "images = os.listdir(path)\n",
        "for image in images:\n",
        "    boxes, pred_cls = get_prediction(path+image, threshold=0.9)\n",
        "    np_boxes = np.array(boxes)\n",
        "    np_classes = np.array(pred_cls)\n",
        "    img_object = []\n",
        "\n",
        "    for class_, box_ in zip(np_classes, np_boxes):\n",
        "      object = {}\n",
        "      object[\"class\"] = class_\n",
        "      object[\"x\"] = int(box_[0][0])\n",
        "      object[\"y\"] = int(box_[0][1])\n",
        "      object[\"width\"] = int(box_[1][0] - box_[0][0])\n",
        "      object[\"height\"] = int(box_[1][1] - box_[0][1])\n",
        "      img_object.append(object)\n",
        "\n",
        "    single_image = {}\n",
        "    single_image[\"index\"] = image.split('.')[0]\n",
        "    single_image[\"objects\"] = img_object\n",
        "    img_data['imgs'].append(single_image)\n",
        "\n",
        "print(img_data)\n",
        "with open('all-people-json.txt', 'w') as outfile:\n",
        "    json.dump(img_data, outfile)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'imgs': [{'index': '00017', 'objects': [{'class': 'person', 'x': 397, 'y': 0, 'width': 417, 'height': 480}, {'class': 'person', 'x': 51, 'y': 38, 'width': 431, 'height': 439}, {'class': 'person', 'x': 766, 'y': 35, 'width': 85, 'height': 433}, {'class': 'car', 'x': 507, 'y': 147, 'width': 10, 'height': 7}]}, {'index': '00022', 'objects': [{'class': 'person', 'x': 363, 'y': 0, 'width': 422, 'height': 480}, {'class': 'person', 'x': 23, 'y': 81, 'width': 423, 'height': 398}, {'class': 'person', 'x': 729, 'y': 23, 'width': 123, 'height': 452}]}, {'index': '00004', 'objects': [{'class': 'person', 'x': 301, 'y': 2, 'width': 499, 'height': 477}, {'class': 'car', 'x': 665, 'y': 155, 'width': 40, 'height': 22}, {'class': 'person', 'x': 59, 'y': 14, 'width': 591, 'height': 461}, {'class': 'car', 'x': 615, 'y': 152, 'width': 34, 'height': 23}, {'class': 'person', 'x': 717, 'y': 8, 'width': 132, 'height': 462}]}, {'index': '00002', 'objects': [{'class': 'person', 'x': 361, 'y': 9, 'width': 384, 'height': 466}, {'class': 'person', 'x': 11, 'y': 92, 'width': 405, 'height': 380}, {'class': 'car', 'x': 680, 'y': 160, 'width': 55, 'height': 29}, {'class': 'person', 'x': 711, 'y': 14, 'width': 137, 'height': 460}, {'class': 'car', 'x': 623, 'y': 157, 'width': 48, 'height': 28}]}, {'index': '00040', 'objects': [{'class': 'person', 'x': 107, 'y': 41, 'width': 302, 'height': 438}, {'class': 'person', 'x': 384, 'y': 0, 'width': 425, 'height': 480}, {'class': 'car', 'x': 614, 'y': 146, 'width': 50, 'height': 48}, {'class': 'person', 'x': 758, 'y': 14, 'width': 92, 'height': 461}]}, {'index': '00011', 'objects': [{'class': 'person', 'x': 352, 'y': 0, 'width': 404, 'height': 480}, {'class': 'person', 'x': 33, 'y': 17, 'width': 379, 'height': 462}, {'class': 'person', 'x': 720, 'y': 5, 'width': 129, 'height': 471}, {'class': 'car', 'x': 619, 'y': 148, 'width': 21, 'height': 19}]}, {'index': '00001', 'objects': [{'class': 'person', 'x': 376, 'y': 23, 'width': 362, 'height': 453}, {'class': 'person', 'x': 51, 'y': 109, 'width': 361, 'height': 370}, {'class': 'person', 'x': 709, 'y': 29, 'width': 140, 'height': 438}, {'class': 'car', 'x': 638, 'y': 180, 'width': 56, 'height': 33}, {'class': 'car', 'x': 700, 'y': 185, 'width': 41, 'height': 32}]}, {'index': '00034', 'objects': [{'class': 'person', 'x': 72, 'y': 35, 'width': 374, 'height': 444}, {'class': 'person', 'x': 397, 'y': 2, 'width': 425, 'height': 477}, {'class': 'car', 'x': 628, 'y': 161, 'width': 29, 'height': 30}, {'class': 'person', 'x': 767, 'y': 142, 'width': 85, 'height': 322}]}, {'index': '00021', 'objects': [{'class': 'person', 'x': 370, 'y': 0, 'width': 419, 'height': 479}, {'class': 'person', 'x': 8, 'y': 72, 'width': 416, 'height': 407}, {'class': 'person', 'x': 735, 'y': 23, 'width': 116, 'height': 450}, {'class': 'car', 'x': 493, 'y': 154, 'width': 9, 'height': 7}]}, {'index': '00038', 'objects': [{'class': 'person', 'x': 65, 'y': 49, 'width': 369, 'height': 429}, {'class': 'person', 'x': 377, 'y': 0, 'width': 431, 'height': 480}, {'class': 'car', 'x': 617, 'y': 158, 'width': 34, 'height': 40}, {'class': 'person', 'x': 747, 'y': 20, 'width': 101, 'height': 429}]}, {'index': '00031', 'objects': [{'class': 'person', 'x': 379, 'y': 4, 'width': 452, 'height': 475}, {'class': 'person', 'x': 30, 'y': 53, 'width': 406, 'height': 426}, {'class': 'car', 'x': 612, 'y': 147, 'width': 38, 'height': 38}, {'class': 'person', 'x': 751, 'y': 18, 'width': 99, 'height': 450}]}, {'index': '00037', 'objects': [{'class': 'person', 'x': 60, 'y': 43, 'width': 374, 'height': 435}, {'class': 'person', 'x': 379, 'y': 0, 'width': 462, 'height': 479}, {'class': 'car', 'x': 617, 'y': 160, 'width': 33, 'height': 39}, {'class': 'person', 'x': 747, 'y': 35, 'width': 101, 'height': 425}]}, {'index': '00018', 'objects': [{'class': 'person', 'x': 405, 'y': 0, 'width': 410, 'height': 480}, {'class': 'person', 'x': 57, 'y': 46, 'width': 406, 'height': 433}, {'class': 'person', 'x': 775, 'y': 98, 'width': 75, 'height': 360}, {'class': 'car', 'x': 513, 'y': 150, 'width': 9, 'height': 7}]}, {'index': '00042', 'objects': [{'class': 'person', 'x': 98, 'y': 27, 'width': 300, 'height': 452}, {'class': 'person', 'x': 392, 'y': 0, 'width': 429, 'height': 480}, {'class': 'car', 'x': 626, 'y': 144, 'width': 48, 'height': 48}, {'class': 'person', 'x': 760, 'y': 20, 'width': 91, 'height': 442}]}, {'index': '00036', 'objects': [{'class': 'person', 'x': 45, 'y': 32, 'width': 395, 'height': 443}, {'class': 'person', 'x': 388, 'y': 2, 'width': 434, 'height': 477}, {'class': 'car', 'x': 621, 'y': 162, 'width': 32, 'height': 35}, {'class': 'person', 'x': 751, 'y': 69, 'width': 98, 'height': 357}]}, {'index': '00023', 'objects': [{'class': 'person', 'x': 382, 'y': 2, 'width': 444, 'height': 477}, {'class': 'person', 'x': 18, 'y': 77, 'width': 435, 'height': 401}, {'class': 'person', 'x': 736, 'y': 17, 'width': 116, 'height': 455}]}, {'index': '00015', 'objects': [{'class': 'person', 'x': 391, 'y': 4, 'width': 431, 'height': 465}, {'class': 'person', 'x': 52, 'y': 68, 'width': 386, 'height': 409}, {'class': 'person', 'x': 757, 'y': 42, 'width': 94, 'height': 434}, {'class': 'car', 'x': 507, 'y': 157, 'width': 10, 'height': 7}, {'class': 'truck', 'x': 517, 'y': 153, 'width': 30, 'height': 35}]}, {'index': '00014', 'objects': [{'class': 'person', 'x': 388, 'y': 10, 'width': 431, 'height': 469}, {'class': 'person', 'x': 56, 'y': 47, 'width': 400, 'height': 432}, {'class': 'person', 'x': 747, 'y': 63, 'width': 105, 'height': 400}, {'class': 'car', 'x': 501, 'y': 159, 'width': 10, 'height': 8}]}, {'index': '00027', 'objects': [{'class': 'person', 'x': 364, 'y': 0, 'width': 438, 'height': 479}, {'class': 'person', 'x': 26, 'y': 120, 'width': 398, 'height': 359}, {'class': 'car', 'x': 594, 'y': 151, 'width': 40, 'height': 38}, {'class': 'person', 'x': 737, 'y': 6, 'width': 113, 'height': 464}]}, {'index': '00005', 'objects': [{'class': 'car', 'x': 659, 'y': 159, 'width': 42, 'height': 21}, {'class': 'person', 'x': 306, 'y': 0, 'width': 491, 'height': 479}, {'class': 'person', 'x': 44, 'y': 29, 'width': 583, 'height': 450}, {'class': 'person', 'x': 724, 'y': 13, 'width': 124, 'height': 461}, {'class': 'car', 'x': 616, 'y': 157, 'width': 29, 'height': 21}]}, {'index': '00012', 'objects': [{'class': 'person', 'x': 369, 'y': 5, 'width': 405, 'height': 474}, {'class': 'person', 'x': 37, 'y': 40, 'width': 386, 'height': 439}, {'class': 'person', 'x': 718, 'y': 32, 'width': 132, 'height': 440}, {'class': 'car', 'x': 625, 'y': 158, 'width': 13, 'height': 17}, {'class': 'car', 'x': 489, 'y': 150, 'width': 10, 'height': 8}]}, {'index': '00016', 'objects': [{'class': 'person', 'x': 391, 'y': 5, 'width': 417, 'height': 474}, {'class': 'person', 'x': 27, 'y': 50, 'width': 433, 'height': 429}, {'class': 'person', 'x': 762, 'y': 46, 'width': 87, 'height': 426}]}, {'index': '00007', 'objects': [{'class': 'car', 'x': 654, 'y': 160, 'width': 36, 'height': 21}, {'class': 'person', 'x': 322, 'y': 5, 'width': 516, 'height': 474}, {'class': 'person', 'x': 60, 'y': 25, 'width': 604, 'height': 454}]}, {'index': '00013', 'objects': [{'class': 'person', 'x': 371, 'y': 3, 'width': 401, 'height': 476}, {'class': 'person', 'x': 39, 'y': 33, 'width': 418, 'height': 446}, {'class': 'person', 'x': 725, 'y': 48, 'width': 126, 'height': 422}, {'class': 'car', 'x': 495, 'y': 154, 'width': 10, 'height': 8}]}, {'index': '00024', 'objects': [{'class': 'person', 'x': 381, 'y': 4, 'width': 407, 'height': 475}, {'class': 'person', 'x': 6, 'y': 104, 'width': 415, 'height': 372}, {'class': 'person', 'x': 733, 'y': 19, 'width': 119, 'height': 456}]}, {'index': '00030', 'objects': [{'class': 'person', 'x': 371, 'y': 4, 'width': 443, 'height': 475}, {'class': 'person', 'x': 22, 'y': 72, 'width': 406, 'height': 407}, {'class': 'person', 'x': 737, 'y': 13, 'width': 113, 'height': 463}, {'class': 'car', 'x': 604, 'y': 148, 'width': 40, 'height': 39}]}, {'index': '00026', 'objects': [{'class': 'person', 'x': 371, 'y': 1, 'width': 446, 'height': 474}, {'class': 'person', 'x': 7, 'y': 109, 'width': 421, 'height': 359}, {'class': 'car', 'x': 598, 'y': 157, 'width': 32, 'height': 33}, {'class': 'person', 'x': 726, 'y': 4, 'width': 122, 'height': 470}]}, {'index': '00009', 'objects': [{'class': 'person', 'x': 324, 'y': 3, 'width': 512, 'height': 476}, {'class': 'car', 'x': 645, 'y': 155, 'width': 35, 'height': 21}, {'class': 'person', 'x': 53, 'y': 16, 'width': 478, 'height': 460}]}, {'index': '00041', 'objects': [{'class': 'person', 'x': 370, 'y': 0, 'width': 423, 'height': 480}, {'class': 'person', 'x': 99, 'y': 42, 'width': 324, 'height': 433}, {'class': 'car', 'x': 619, 'y': 147, 'width': 49, 'height': 47}, {'class': 'person', 'x': 760, 'y': 15, 'width': 91, 'height': 453}]}, {'index': '00000', 'objects': [{'class': 'person', 'x': 354, 'y': 19, 'width': 370, 'height': 459}, {'class': 'person', 'x': 692, 'y': 18, 'width': 157, 'height': 454}, {'class': 'car', 'x': 627, 'y': 174, 'width': 59, 'height': 34}, {'class': 'person', 'x': 26, 'y': 130, 'width': 375, 'height': 348}]}, {'index': '00039', 'objects': [{'class': 'person', 'x': 100, 'y': 41, 'width': 311, 'height': 438}, {'class': 'person', 'x': 374, 'y': 2, 'width': 447, 'height': 473}, {'class': 'car', 'x': 610, 'y': 149, 'width': 48, 'height': 46}, {'class': 'person', 'x': 751, 'y': 16, 'width': 98, 'height': 460}]}, {'index': '00019', 'objects': [{'class': 'person', 'x': 398, 'y': 0, 'width': 422, 'height': 480}, {'class': 'person', 'x': 38, 'y': 44, 'width': 429, 'height': 435}, {'class': 'car', 'x': 511, 'y': 153, 'width': 11, 'height': 8}, {'class': 'person', 'x': 767, 'y': 43, 'width': 83, 'height': 426}]}, {'index': '00033', 'objects': [{'class': 'person', 'x': 87, 'y': 36, 'width': 369, 'height': 443}, {'class': 'person', 'x': 379, 'y': 4, 'width': 452, 'height': 475}, {'class': 'car', 'x': 628, 'y': 149, 'width': 32, 'height': 37}, {'class': 'person', 'x': 770, 'y': 131, 'width': 82, 'height': 326}]}, {'index': '00035', 'objects': [{'class': 'person', 'x': 57, 'y': 32, 'width': 388, 'height': 447}, {'class': 'person', 'x': 380, 'y': 0, 'width': 420, 'height': 480}, {'class': 'person', 'x': 761, 'y': 77, 'width': 90, 'height': 388}, {'class': 'car', 'x': 626, 'y': 161, 'width': 30, 'height': 36}]}, {'index': '00029', 'objects': [{'class': 'person', 'x': 371, 'y': 2, 'width': 405, 'height': 477}, {'class': 'person', 'x': 19, 'y': 97, 'width': 413, 'height': 382}, {'class': 'car', 'x': 602, 'y': 149, 'width': 41, 'height': 39}, {'class': 'person', 'x': 740, 'y': 19, 'width': 112, 'height': 459}]}, {'index': '00003', 'objects': [{'class': 'person', 'x': 356, 'y': 3, 'width': 362, 'height': 474}, {'class': 'person', 'x': 1, 'y': 86, 'width': 421, 'height': 388}, {'class': 'car', 'x': 670, 'y': 153, 'width': 51, 'height': 27}, {'class': 'person', 'x': 709, 'y': 21, 'width': 140, 'height': 449}, {'class': 'car', 'x': 612, 'y': 150, 'width': 47, 'height': 26}]}, {'index': '00020', 'objects': [{'class': 'person', 'x': 384, 'y': 3, 'width': 431, 'height': 476}, {'class': 'person', 'x': 31, 'y': 64, 'width': 429, 'height': 415}, {'class': 'person', 'x': 752, 'y': 21, 'width': 100, 'height': 447}]}, {'index': '00025', 'objects': [{'class': 'person', 'x': 384, 'y': 0, 'width': 410, 'height': 480}, {'class': 'person', 'x': 10, 'y': 99, 'width': 414, 'height': 376}, {'class': 'person', 'x': 727, 'y': 17, 'width': 123, 'height': 458}]}, {'index': '00028', 'objects': [{'class': 'person', 'x': 366, 'y': 3, 'width': 424, 'height': 476}, {'class': 'person', 'x': 28, 'y': 123, 'width': 406, 'height': 356}, {'class': 'person', 'x': 738, 'y': 12, 'width': 114, 'height': 459}, {'class': 'car', 'x': 592, 'y': 149, 'width': 42, 'height': 38}]}, {'index': '00010', 'objects': [{'class': 'person', 'x': 305, 'y': 2, 'width': 522, 'height': 477}, {'class': 'car', 'x': 645, 'y': 151, 'width': 33, 'height': 21}, {'class': 'person', 'x': 58, 'y': 21, 'width': 502, 'height': 456}]}, {'index': '00032', 'objects': [{'class': 'person', 'x': 383, 'y': 0, 'width': 432, 'height': 480}, {'class': 'person', 'x': 61, 'y': 47, 'width': 376, 'height': 432}, {'class': 'car', 'x': 620, 'y': 147, 'width': 33, 'height': 39}, {'class': 'person', 'x': 758, 'y': 18, 'width': 93, 'height': 456}]}, {'index': '00006', 'objects': [{'class': 'car', 'x': 658, 'y': 161, 'width': 38, 'height': 22}, {'class': 'person', 'x': 332, 'y': 6, 'width': 467, 'height': 473}, {'class': 'person', 'x': 66, 'y': 38, 'width': 497, 'height': 437}, {'class': 'person', 'x': 720, 'y': 17, 'width': 130, 'height': 451}, {'class': 'car', 'x': 621, 'y': 161, 'width': 23, 'height': 20}]}, {'index': '00008', 'objects': [{'class': 'car', 'x': 652, 'y': 159, 'width': 35, 'height': 21}, {'class': 'person', 'x': 333, 'y': 2, 'width': 501, 'height': 477}, {'class': 'person', 'x': 52, 'y': 27, 'width': 545, 'height': 452}]}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKHqJVjOQHM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "path = './People/'\n",
        "memmaps = []\n",
        "\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "    memmap_filename = filename.split('.')[0]+'.mmap'\n",
        "    memmap_filename = path+'memmap/'+memmap_filename\n",
        "    os.makedirs(os.path.dirname(memmap_filename), exist_ok=True)\n",
        "\n",
        "    im = cv2.imread(path+filename)\n",
        "    if not (im is None):\n",
        "      dtype = im.dtype\n",
        "      shape = im.shape\n",
        "      fp = np.memmap(memmap_filename, dtype=dtype, mode='w+', shape=shape)\n",
        "      fp[:] = im[:]\n",
        "      memmaps.append(fp)\n",
        "\n",
        "complete_memmap = path + 'memmap/'+'all-people.mmap'\n",
        "\n",
        "os.makedirs(os.path.dirname(path + 'memmap'), exist_ok=True)\n",
        "complete_shape = (len(memmaps), shape[0], shape[1], shape[2])\n",
        "    # print(\"Complete shape \",complete_shape) #(82, 480, 854, 3)\n",
        "    # complete_shape=(82,480,854,3)\n",
        "    # print(complete_shape)\n",
        "fpc = np.memmap(complete_memmap, dtype=dtype, mode='w+', shape=complete_shape)\n",
        "fpc[:] = memmaps[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGHTd-zVVdVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "90be24e2-b016-4014-d22c-ae242a280f62"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "\n",
        "class DavisDataset(Dataset):\n",
        "\n",
        "    def __init__(self, memmap, json_data, need_classes):\n",
        "\n",
        "        ##cropping objects from memaps and add to idxes\n",
        "        self.memmap = memmap\n",
        "        self.json_data = json_data\n",
        "        self.idxes = {}\n",
        "        self.max_no_classes = 0\n",
        "        self.need_classes = need_classes\n",
        "\n",
        "        imgs = self.json_data['imgs']\n",
        "        for img in imgs:\n",
        "            idx = int(img['index'])\n",
        "            classes = img['objects']\n",
        "            classes_ = []\n",
        "            objects_ = []\n",
        "            class_objects = []\n",
        "            for class_idx, class_ in enumerate(classes):\n",
        "              if class_['class']  in self.need_classes:\n",
        "                  single_class = class_['class']\n",
        "                  x = class_['x']\n",
        "                  y = class_['y']\n",
        "                  width = class_['width']\n",
        "                  height = class_['height']\n",
        "\n",
        "                  if class_idx > self.max_no_classes:\n",
        "                      self.max_no_classes = class_idx\n",
        "\n",
        "                  image = self.memmap[idx]\n",
        "                  crop_img = image[y:y + height, x:x + width]\n",
        "                  img_resized = cv2.resize(crop_img, (200,200), interpolation = cv2.INTER_AREA)\n",
        "                  classes_.append(single_class)\n",
        "                  objects_.append(self.image_to_tensor(img_resized))\n",
        "                  #objects_.append(img_resized)\n",
        "\n",
        "            class_objects.append(classes_)\n",
        "            class_objects.append(objects_)\n",
        "            self.idxes[idx] = class_objects\n",
        "\n",
        "    def image_to_tensor(self,image, mean=0, std=1.):\n",
        "        image = image.astype(np.float32)\n",
        "        image = (image - mean) / std\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        tensor = torch.from_numpy(image)\n",
        "        return tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memmap)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        classes, images = self.idxes.get(index)\n",
        "        return [classes, images]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                                  )\n",
        "        # output 64, 47, 47\n",
        "        self.fc = nn.Sequential(nn.Linear(64 * 47 * 47, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 256)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        z = self.fc(x)\n",
        "        return z\n",
        "\n",
        "#loading memmap\n",
        "folder_path='./drive/My Drive/Thesis_2020/'\n",
        "complete_shape=(43,480,854,3)\n",
        "complete_memmap = folder_path+'all-people.mmap'\n",
        "newfp = np.memmap(complete_memmap, dtype='uint8', mode='r', shape=complete_shape)\n",
        "\n",
        "#loading json\n",
        "with open(folder_path+'all-people-json.txt') as json_file:\n",
        "    data_json = json.load(json_file)\n",
        "\n",
        "def show_objects(classes,images):\n",
        "  img_np = []\n",
        "  for i in range(len(classes)):\n",
        "    img_np_i = np.asarray(images[i])\n",
        "    img_np.append(img_np_i)\n",
        "\n",
        "  f, axarr = plt.subplots(1, len(classes))\n",
        "  for i in range(len(classes)):\n",
        "    axarr[i].imshow(img_np[i], interpolation='nearest')\n",
        "  plt.show()\n",
        "\n",
        "need_classes = ['person']\n",
        "davisdt = DavisDataset(newfp, data_json, need_classes)\n",
        "\n",
        "dataset_loader = torch.utils.data.DataLoader(davisdt, batch_size=1, shuffle=True)\n",
        "\n",
        "encoder = Encoder()\n",
        "one_image_zs = []\n",
        "\n",
        "for batch in dataset_loader:\n",
        "    class_label , images = batch[0], batch[1]\n",
        "    single_block = []\n",
        "\n",
        "    #store z of each image objects per image \n",
        "    for i in range(len(class_label)):\n",
        "      single_block.append([class_label[i],images[i],encoder(images[i])])\n",
        "    one_image_zs.append(single_block)\n",
        "\n",
        "    \n",
        "    object_zs_same = []\n",
        "\n",
        "    #store main Z and avg Z per image\n",
        "    for one_img_z in one_image_zs:\n",
        "      no_objects = len(one_img_z)\n",
        "      all_zs_in_one_image = [one_img_z[i][2] for i in range(no_objects)]\n",
        "      for index in range(no_objects):\n",
        "        main_z = all_zs_in_one_image[index]\n",
        "        contexr_avg_z = torch.mean(all_zs_in_one_image)\n",
        "        print(contexr_avg_z)\n",
        "        #print(main_z.shape, contexr_avg_z.shape)\n",
        "        object_zs_same.append([main_z,contexr_avg_z])\n",
        "      #print(object_zs_same)\n",
        " \n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def show_images():\n",
        "  for i in range(43):\n",
        "    classes, images  = davisdt.__getitem__(i)\n",
        "    print('index : ', i, classes)\n",
        "    show_objects(classes,images)\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-286ffce4893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mmain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_zs_in_one_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mcontexr_avg_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_zs_in_one_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexr_avg_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m#print(main_z.shape, contexr_avg_z.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ]
    }
  ]
}